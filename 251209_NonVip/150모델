import os
import glob
import numpy as np
import pandas as pd
from scipy.stats import linregress

# sklearn ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìˆ˜ë™ min-max
try:
    from sklearn.preprocessing import MinMaxScaler
    SKLEARN_OK = True
except Exception:
    SKLEARN_OK = False

print("ğŸ“‚ CSV ìë™ íƒìƒ‰(ì»¬ëŸ¼ ê¸°ë°˜) ì‹œì‘...")

# =========================
# 1) í´ë” CSV ëª©ë¡
# =========================
csv_files = sorted(glob.glob("*.csv"))
if len(csv_files) == 0:
    raise FileNotFoundError("âŒ í˜„ì¬ í´ë”ì— CSV íŒŒì¼ì´ ì—†ìŒ")

print(f"âœ… ë°œê²¬ëœ CSV: {len(csv_files)}ê°œ")

# =========================
# 2) CSV í—¤ë”(ì»¬ëŸ¼)ë§Œ ì•ˆì „í•˜ê²Œ ì½ëŠ” í•¨ìˆ˜
# =========================
def read_columns_only(path):
    encodings = ["utf-8-sig", "utf-8", "cp949", "euc-kr"]
    last_err = None
    for enc in encodings:
        try:
            tmp = pd.read_csv(path, nrows=0, encoding=enc, low_memory=False)
            return list(tmp.columns), enc
        except Exception as e:
            last_err = e
            continue
    return None, None

# =========================
# 3) ì»¬ëŸ¼ ìë™ ì°¾ê¸°(í‚¤ì›Œë“œ ë§¤ì¹­)
# =========================
def find_col(cols, exact_first=None, must_have=None, any_have=None):
    cols_list = list(cols)

    if exact_first is None:
        exact_first = []
    if must_have is None:
        must_have = []
    if any_have is None:
        any_have = []

    for ex in exact_first:
        if ex in cols_list:
            return ex

    if len(must_have) > 0:
        for c in cols_list:
            ok = True
            for kw in must_have:
                if kw not in c:
                    ok = False
                    break
            if ok:
                return c

    if len(any_have) > 0:
        for c in cols_list:
            for kw in any_have:
                if kw in c:
                    return c

    return None

# =========================
# 4) ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ìë™ ì„ íƒ
#    (ë³€ìˆ˜ëª… ì»¬ëŸ¼ ìˆê±°ë‚˜, 150ê°œ ë‚´ì™¸ë©´ ìš°ì„ )
# =========================
feature_candidates = []
for f in csv_files:
    cols, enc = read_columns_only(f)
    if cols is None:
        continue

    score = 0
    name_l = f.lower()

    if "final_selected_150" in name_l:
        score += 50
    if "150" in name_l:
        score += 10
    if ("feature" in name_l) or ("ë³€ìˆ˜" in f):
        score += 5
    if "ë³€ìˆ˜ëª…" in cols:
        score += 30

    # shap importance ê°™ì€ ì• ë“¤ì€ ê°ì (ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì“°ê¸° ì• ë§¤)
    if "shap_values" in name_l or "feature_importance" in name_l:
        score -= 20

    feature_candidates.append((score, f, enc, cols))

feature_candidates = sorted(feature_candidates, key=lambda x: x[0], reverse=True)
if len(feature_candidates) == 0 or feature_candidates[0][0] <= 0:
    raise FileNotFoundError("âŒ ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì“¸ë§Œí•œ CSVë¥¼ ìë™ìœ¼ë¡œ ëª» ì°¾ìŒ")

feature_file = feature_candidates[0][1]
print(f"âœ… ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ìë™ ì„ íƒ: {feature_file}")

# ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
try:
    features_df = pd.read_csv(feature_file, encoding="utf-8-sig")
except Exception:
    try:
        features_df = pd.read_csv(feature_file, encoding="cp949")
    except Exception:
        features_df = pd.read_csv(feature_file)

if "ë³€ìˆ˜ëª…" in features_df.columns:
    selected_features = features_df["ë³€ìˆ˜ëª…"].dropna().astype(str).tolist()
else:
    selected_features = features_df.iloc[:, 0].dropna().astype(str).tolist()

print(f"âœ… ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ ê°œìˆ˜: {len(selected_features)}ê°œ")

# =========================
# 5) VIP ì›ë³¸(ë°ì´í„°) íŒŒì¼ ìë™ ì„ íƒ
#    -> íŒŒì¼ëª…ì´ ì•„ë‹ˆë¼ "í•„ìˆ˜ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€"ë¡œ ì ìˆ˜ ë§¤ê¹€
# =========================
data_candidates = []

for f in csv_files:
    if f == feature_file:
        continue

    cols, enc = read_columns_only(f)
    if cols is None:
        continue

    name_l = f.lower()

    id_col = find_col(cols, exact_first=["ë°œê¸‰íšŒì›ë²ˆí˜¸"], must_have=["íšŒì›", "ë²ˆí˜¸"], any_have=["íšŒì›ë²ˆí˜¸", "ê³ ê°ë²ˆí˜¸"])
    month_col = find_col(cols, exact_first=["ê¸°ì¤€ë…„ì›”"], any_have=["ë…„ì›”", "ê¸°ì¤€ì›”", "ê¸°ì¤€_ë…„ì›”"])
    spend_col = find_col(cols, exact_first=["ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M"], must_have=["ì´ìš©ê¸ˆì•¡"], any_have=["ì´ìš©ê¸ˆì•¡"])
    count_col = find_col(cols, exact_first=["ì´ìš©ê±´ìˆ˜_ì‹ ìš©_B0M"], must_have=["ì´ìš©ê±´ìˆ˜"], any_have=["ì´ìš©ê±´ìˆ˜", "ìŠ¹ì¸ê±´ìˆ˜"])
    bal_col = find_col(cols, exact_first=["ì”ì•¡_B0M"], must_have=["ì”ì•¡"], any_have=["ì”ì•¡"])

    score = 0
    if id_col is not None: score += 30
    if month_col is not None: score += 30
    if spend_col is not None: score += 15
    if count_col is not None: score += 15
    if bal_col is not None: score += 10

    # ì»¬ëŸ¼ ë§ì„ìˆ˜ë¡(ì›ë³¸ì¼ í™•ë¥ ) ê°€ì‚°
    if len(cols) >= 200: score += 10
    if len(cols) >= 500: score += 10

    # íŒŒì¼ëª… íŒíŠ¸ ê°€ì‚°/ê°ì 
    if "ë°ì´í„°" in f or "final" in name_l: score += 5
    if "shap_values" in name_l or "feature_importance" in name_l: score -= 30
    if "ì„ íƒë³€ìˆ˜ë¦¬ìŠ¤íŠ¸" in f: score -= 40

    data_candidates.append((score, f, enc, id_col, month_col, spend_col, count_col, bal_col))

data_candidates = sorted(data_candidates, key=lambda x: x[0], reverse=True)

print("\nğŸ” ë°ì´í„° íŒŒì¼ í›„ë³´ TOP5(ì ìˆ˜, íŒŒì¼ëª…)")
for item in data_candidates[:5]:
    print(f" - {item[0]:>3}ì  | {item[1]}")

if len(data_candidates) == 0 or data_candidates[0][0] < 60:
    raise FileNotFoundError("âŒ 'ë°œê¸‰íšŒì›ë²ˆí˜¸/ê¸°ì¤€ë…„ì›”/ì´ìš©ê¸ˆì•¡/ì´ìš©ê±´ìˆ˜/ì”ì•¡' êµ¬ì„±ëœ ë°ì´í„° íŒŒì¼ì„ ìë™ìœ¼ë¡œ ëª» ì°¾ìŒ")

vip_file = data_candidates[0][1]
ENC_DATA = data_candidates[0][2]

ID_COL = data_candidates[0][3]
MONTH_COL = data_candidates[0][4]
SPEND_COL = data_candidates[0][5]
COUNT_COL = data_candidates[0][6]
BAL_COL = data_candidates[0][7]

print(f"\nâœ… ë°ì´í„° íŒŒì¼ ìë™ ì„ íƒ: {vip_file}")
print("ğŸ§© ìë™ ì¸ì‹ ì»¬ëŸ¼")
print(f" - ID      : {ID_COL}")
print(f" - YEARMO  : {MONTH_COL}")
print(f" - SPEND   : {SPEND_COL}")
print(f" - COUNT   : {COUNT_COL}")
print(f" - BALANCE : {BAL_COL}")

# =========================
# 6) ë°ì´í„° ë¡œë“œ
# =========================
print("\nğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
try:
    df = pd.read_csv(vip_file, encoding=ENC_DATA, low_memory=False)
except Exception:
    df = pd.read_csv(vip_file, low_memory=False)

print(f"âœ… ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")

# ë¦¬ìŠ¤í¬ ì»¬ëŸ¼ì€ ìˆìœ¼ë©´ ì“°ê³ , ì—†ìœ¼ë©´ 0 ì²˜ë¦¬
RISK_BAL_COL = "ì—°ì²´ì”ì•¡_B0M" if "ì—°ì²´ì”ì•¡_B0M" in df.columns else None
RISK_DECLINE_COL = "ìŠ¹ì¸ê±°ì ˆê±´ìˆ˜_B0M" if "ìŠ¹ì¸ê±°ì ˆê±´ìˆ˜_B0M" in df.columns else None

# íƒ€ì… ì •ë¦¬
df[MONTH_COL] = df[MONTH_COL].astype(str).str.replace(".0", "", regex=False)
df[MONTH_COL] = pd.to_numeric(df[MONTH_COL], errors="coerce")
df = df.dropna(subset=[ID_COL, MONTH_COL])
df[MONTH_COL] = df[MONTH_COL].astype(int)

for c in [SPEND_COL, COUNT_COL, BAL_COL]:
    df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)

if RISK_BAL_COL is None:
    df["__RISK_BAL_TMP__"] = 0
    RISK_BAL_COL = "__RISK_BAL_TMP__"
else:
    df[RISK_BAL_COL] = pd.to_numeric(df[RISK_BAL_COL], errors="coerce").fillna(0)

if RISK_DECLINE_COL is None:
    df["__RISK_DEC_TMP__"] = 0
    RISK_DECLINE_COL = "__RISK_DEC_TMP__"
else:
    df[RISK_DECLINE_COL] = pd.to_numeric(df[RISK_DECLINE_COL], errors="coerce").fillna(0)

# =========================
# 7) Churn_Score ê³„ì‚° (ìµœê·¼ 6ê°œì›” slope)
# =========================
print("\nâš¡ Churn_Score ê³„ì‚° ì‹œì‘...")

LOOKBACK_N = 6
W_SPEND, W_BALANCE, W_COUNT, W_RISK = 40, 30, 20, 10

df = df.sort_values([ID_COL, MONTH_COL])
target_month = int(df[MONTH_COL].max())
df_final = df[df[MONTH_COL] == target_month].copy()

print(f"   - ë¶„ì„ ê¸°ì¤€ì›”: {target_month} (ê³ ê° ìˆ˜: {len(df_final)}ëª…)")

def calc_slope_last_n(group_df, value_col, n):
    s = group_df[value_col].tail(n).fillna(0)
    if len(s) < 2:
        return 0.0
    if float(s.sum()) == 0.0:
        return 0.0
    y = s.values.astype(float)
    if np.all(y == y[0]):
        return 0.0
    x = np.arange(len(y))
    slope, _, _, _, _ = linregress(x, y)
    if np.isnan(slope):
        return 0.0
    return float(slope)

print("   - ê³ ê°ë³„ ìµœê·¼ 6ê°œì›” ê¸°ìš¸ê¸° ê³„ì‚° ì¤‘...")
slope_spend = df.groupby(ID_COL, sort=False).apply(lambda g: calc_slope_last_n(g, SPEND_COL, LOOKBACK_N))
slope_count = df.groupby(ID_COL, sort=False).apply(lambda g: calc_slope_last_n(g, COUNT_COL, LOOKBACK_N))
slope_bal   = df.groupby(ID_COL, sort=False).apply(lambda g: calc_slope_last_n(g, BAL_COL, LOOKBACK_N))

df_final["Slope_Spend"] = df_final[ID_COL].map(slope_spend).fillna(0)
df_final["Slope_Count"] = df_final[ID_COL].map(slope_count).fillna(0)
df_final["Slope_Balance"] = df_final[ID_COL].map(slope_bal).fillna(0)

def slope_to_score(slope_series):
    neg = slope_series.apply(lambda x: -x if x < 0 else 0).astype(float).values
    if np.max(neg) == np.min(neg):
        return np.zeros(len(neg))
    if SKLEARN_OK:
        scaler = MinMaxScaler()
        return scaler.fit_transform(neg.reshape(-1, 1)).flatten()
    return (neg - np.min(neg)) / (np.max(neg) - np.min(neg))

score_spend = slope_to_score(df_final["Slope_Spend"])
score_count = slope_to_score(df_final["Slope_Count"])
score_balance = slope_to_score(df_final["Slope_Balance"])

has_risk = np.where(
    (df_final[RISK_BAL_COL] > 0) | (df_final[RISK_DECLINE_COL] > 0),
    1,
    0
)

df_final["Churn_Score"] = (
    (score_spend * W_SPEND) +
    (score_balance * W_BALANCE) +
    (score_count * W_COUNT) +
    (has_risk * W_RISK)
).round(1)

print(f"âœ… Churn_Score ìƒì„± ì™„ë£Œ! (í‰ê· : {df_final['Churn_Score'].mean():.1f}ì )")

# =========================
# 8) 150ê°œ ë³€ìˆ˜ + ì ìˆ˜ ì €ì¥
# =========================
print("\nâœ‚ï¸ 150ê°œ ë³€ìˆ˜ + Churn_Score ì¶”ì¶œ/ì €ì¥ ì¤‘...")

selected_in_data = [c for c in selected_features if c in df_final.columns]
missing = [c for c in selected_features if c not in df_final.columns]

print(f"   - ì‹¤ì œ í¬í•¨ëœ ë³€ìˆ˜: {len(selected_in_data)}ê°œ")
print(f"   - ëˆ„ë½ ë³€ìˆ˜: {len(missing)}ê°œ")

final_cols = [ID_COL] + selected_in_data + ["Churn_Score"]
df_export = df_final[final_cols].copy()

OUTPUT_FILE = "VIP_Target_150_Selected.csv"
df_export.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")

print("\nğŸ‰ ì™„ë£Œ!")
print(f"ğŸ“‚ ì €ì¥ íŒŒì¼: {OUTPUT_FILE}")
print(f"ğŸ“Š ìµœì¢… í¬ê¸°: {df_export.shape}")




ğŸ“‚ CSV ìë™ íƒìƒ‰(ì»¬ëŸ¼ ê¸°ë°˜) ì‹œì‘...
âœ… ë°œê²¬ëœ CSV: 13ê°œ
âœ… ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ìë™ ì„ íƒ: Final_Selected_150_Features_SHAP_Priority.csv
âœ… ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ ê°œìˆ˜: 150ê°œ

ğŸ” ë°ì´í„° íŒŒì¼ í›„ë³´ TOP5(ì ìˆ˜, íŒŒì¼ëª…)
 - 125ì  | ì¼ë°˜ì¸ë°ì´í„°.csv
 - 115ì  | ì¼ë°˜ì¸ë°ì´í„°_SHAP0ì œê±°_full.csv
 - 105ì  | ì¼ë°˜ì¸ë°ì´í„°_SHAPVIF_TOP150_ë°ì´í„°.csv
 -  90ì  | shap_values_by_sample.csv
 -  85ì  | ì¼ë°˜ì¸ë°ì´í„°_SHAP0ì œê±°_X_test.csv

âœ… ë°ì´í„° íŒŒì¼ ìë™ ì„ íƒ: ì¼ë°˜ì¸ë°ì´í„°.csv
ğŸ§© ìë™ ì¸ì‹ ì»¬ëŸ¼
 - ID      : ë°œê¸‰íšŒì›ë²ˆí˜¸
 - YEARMO  : ê¸°ì¤€ë…„ì›”
 - SPEND   : ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M
 - COUNT   : ì´ìš©ê±´ìˆ˜_ì‹ ìš©_B0M
 - BALANCE : ì”ì•¡_B0M

ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...
âœ… ì›ë³¸ ë°ì´í„° í¬ê¸°: (48540, 898)

âš¡ Churn_Score ê³„ì‚° ì‹œì‘...
   - ë¶„ì„ ê¸°ì¤€ì›”: 201812 (ê³ ê° ìˆ˜: 8090ëª…)
   - ê³ ê°ë³„ ìµœê·¼ 6ê°œì›” ê¸°ìš¸ê¸° ê³„ì‚° ì¤‘...
C:\Users\82104\AppData\Local\Temp\ipykernel_9236\822094121.py:258: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  slope_spend = df.groupby(ID_COL, sort=False).apply(lambda g: calc_slope_last_n(g, SPEND_COL, LOOKBACK_N))
C:\Users\82104\AppData\Local\Temp\ipykernel_9236\822094121.py:259: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  slope_count = df.groupby(ID_COL, sort=False).apply(lambda g: calc_slope_last_n(g, COUNT_COL, LOOKBACK_N))
C:\Users\82104\AppData\Local\Temp\ipykernel_9236\822094121.py:260: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.
  slope_bal   = df.groupby(ID_COL, sort=False).apply(lambda g: calc_slope_last_n(g, BAL_COL, LOOKBACK_N))
âœ… Churn_Score ìƒì„± ì™„ë£Œ! (í‰ê· : 3.9ì )

âœ‚ï¸ 150ê°œ ë³€ìˆ˜ + Churn_Score ì¶”ì¶œ/ì €ì¥ ì¤‘...
   - ì‹¤ì œ í¬í•¨ëœ ë³€ìˆ˜: 150ê°œ
   - ëˆ„ë½ ë³€ìˆ˜: 0ê°œ

ğŸ‰ ì™„ë£Œ!
ğŸ“‚ ì €ì¥ íŒŒì¼: VIP_Target_150_Selected.csv
ğŸ“Š ìµœì¢… í¬ê¸°: (8090, 152)
