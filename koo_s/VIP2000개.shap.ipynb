{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efcd3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/sangrimkoo/Desktop/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„‰á…¡á„‹á…­á†¼á„’á…¡á„‚á…³á†«á„ƒá…¦á„‹á…µá„á…¥/VIPë°ì´í„°.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28ac65bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['ë°œê¸‰íšŒì›ë²ˆí˜¸', 'ê¸°ì¤€ë…„ì›”'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed21bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_slope_6m(series):\n",
    "    # ë°ì´í„°ê°€ 2ê°œ ë¯¸ë§Œì´ê±°ë‚˜ ì „ë¶€ 0ì´ë©´ ê³„ì‚° ë¶ˆê°€ (0 ë°˜í™˜)\n",
    "    if len(series) < 2 or np.sum(series) == 0:\n",
    "        return 0\n",
    "    \n",
    "    y = series.values\n",
    "    x = np.arange(len(y))\n",
    "    \n",
    "    # ê°’ì´ 6ê°œì›” ë‚´ë‚´ ë˜‘ê°™ìœ¼ë©´ ê¸°ìš¸ê¸° 0\n",
    "    if np.all(y == y[0]): return 0\n",
    "    \n",
    "    # ì„ í˜• íšŒê·€ë¡œ ê¸°ìš¸ê¸° ê³„ì‚°\n",
    "    slope, _, _, _, _ = linregress(x, y)\n",
    "    \n",
    "    # ê³„ì‚° ì˜¤ë¥˜(NaN)ê°€ ë‚˜ë©´ 0ìœ¼ë¡œ ì²˜ë¦¬\n",
    "    if np.isnan(slope): return 0\n",
    "    \n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "938b09ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê³ ê°ë³„ ì¶”ì„¸(ê¸°ìš¸ê¸°) ê³„ì‚° ì¤‘... (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆì–´!)\n",
      "ë¶„ì„ ëŒ€ìƒ ì›”: 201812\n"
     ]
    }
   ],
   "source": [
    "print(\"ê³ ê°ë³„ ì¶”ì„¸(ê¸°ìš¸ê¸°) ê³„ì‚° ì¤‘... (ì‹œê°„ì´ ì¢€ ê±¸ë¦´ ìˆ˜ ìˆì–´!)\")\n",
    "\n",
    "# ê° ì§€í‘œë³„ë¡œ 6ê°œì›” ê¸°ìš¸ê¸° ê³„ì‚°\n",
    "df['Slope_Spend'] = df.groupby('ë°œê¸‰íšŒì›ë²ˆí˜¸')['ì´ìš©ê¸ˆì•¡_ì‹ ìš©_B0M'].transform(calc_slope_6m)\n",
    "\n",
    "# ì”ì•¡ ì»¬ëŸ¼ ì´ë¦„ í™•ì¸ í•„ìš” (ë³´í†µ 'ì”ì•¡_í•©ê³„_B0M' ë˜ëŠ” 'ì”ì•¡_B0M')\n",
    "balance_col = 'ì”ì•¡_B0M' if 'ì”ì•¡_B0M' in df.columns else df.columns[df.columns.str.contains('ì”ì•¡')][0]\n",
    "df['Slope_Balance'] = df.groupby('ë°œê¸‰íšŒì›ë²ˆí˜¸')[balance_col].transform(calc_slope_6m)\n",
    "\n",
    "df['Slope_Count'] = df.groupby('ë°œê¸‰íšŒì›ë²ˆí˜¸')['ì´ìš©ê±´ìˆ˜_ì‹ ìš©_B0M'].transform(calc_slope_6m)\n",
    "\n",
    "# ê°€ì¥ ìµœê·¼ ì›”(12ì›”) ë°ì´í„°ë§Œ ì¶”ì¶œ (í˜„ì¬ ì‹œì  ê¸°ì¤€ ë¶„ì„)\n",
    "target_month = df['ê¸°ì¤€ë…„ì›”'].max()\n",
    "df_final = df[df['ê¸°ì¤€ë…„ì›”'] == target_month].copy()\n",
    "\n",
    "print(f\"ë¶„ì„ ëŒ€ìƒ ì›”: {target_month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9487aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "def convert_slope_to_score(df_in, col_name):\n",
    "    # ê°ì†Œ(ìŒìˆ˜)í•˜ëŠ” ê²½ìš°ë§Œ ì¶”ì¶œí•˜ì—¬ ì–‘ìˆ˜ë¡œ ë³€í™˜ (ê°ì†Œí­ì´ í´ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)\n",
    "    # ì¦ê°€í•˜ê±°ë‚˜ ë³€ë™ ì—†ëŠ” ê²½ìš°(0 ì´ìƒ)ëŠ” 0ì ìœ¼ë¡œ ì²˜ë¦¬ (ìœ„í—˜í•˜ì§€ ì•Šìœ¼ë‹ˆê¹Œ!)\n",
    "    negative_slopes = df_in[col_name].apply(lambda x: -x if x < 0 else 0)\n",
    "    \n",
    "    # 0~1 ì‚¬ì´ë¡œ ì ìˆ˜ ì••ì¶• (MinMax Scaling)\n",
    "    return scaler.fit_transform(negative_slopes.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# ê° ì§€í‘œë³„ ì ìˆ˜ ê³„ì‚°\n",
    "score_spend = convert_slope_to_score(df_final, 'Slope_Spend')\n",
    "score_balance = convert_slope_to_score(df_final, 'Slope_Balance')\n",
    "score_count = convert_slope_to_score(df_final, 'Slope_Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d875fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°€ì¤‘ì¹˜ ì„¤ì • (í•©ê³„ 100)\n",
    "W_SPEND = 40   # ì†Œë¹„ ê°ì†Œ ë¹„ì¤‘ (ì œì¼ ì¤‘ìš”!)\n",
    "W_BALANCE = 30 # ì”ì•¡ ê°ì†Œ ë¹„ì¤‘\n",
    "W_COUNT = 20   # ë¹ˆë„ ê°ì†Œ ë¹„ì¤‘\n",
    "W_RISK = 10    # ë¦¬ìŠ¤í¬ ë¹„ì¤‘\n",
    "\n",
    "# ë¦¬ìŠ¤í¬ ì ìˆ˜ (ì—°ì²´ë‚˜ ê±°ì ˆ ì´ë ¥ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0)\n",
    "has_risk = np.where(\n",
    "    (df_final.get('ì—°ì²´ì”ì•¡_B0M', 0) > 0) | \n",
    "    (df_final.get('ìŠ¹ì¸ê±°ì ˆê±´ìˆ˜_B0M', 0) > 0), \n",
    "    1, 0\n",
    ")\n",
    "\n",
    "# ìµœì¢… ì ìˆ˜ ê³„ì‚°\n",
    "df_final['Churn_Score'] = (\n",
    "    (score_spend * W_SPEND) + \n",
    "    (score_balance * W_BALANCE) + \n",
    "    (score_count * W_COUNT) + \n",
    "    (has_risk * W_RISK)\n",
    ")\n",
    "\n",
    "# ë³´ê¸° ì¢‹ê²Œ ì†Œìˆ˜ì  ì²«ì§¸ ìë¦¬ê¹Œì§€ë§Œ ë‚¨ê¹€\n",
    "df_final['Churn_Score'] = df_final['Churn_Score'].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9e2ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Churn_Score ìƒì„± ì™„ë£Œ]\n",
      "ì „ì²´ ê³ ê° ìˆ˜: 1910ëª…\n",
      "ì ìˆ˜ í‰ê· : 8.97ì \n",
      "ì ìˆ˜ ìµœëŒ€ê°’: 63.7ì \n",
      "\n",
      "[ìƒìœ„ 5ëª… ë°ì´í„° í™•ì¸]\n",
      "         ë°œê¸‰íšŒì›ë²ˆí˜¸    Slope_Spend  Slope_Balance  Churn_Score\n",
      "10325  SYN_3968 -243906.428571 -117226.657143         63.7\n",
      "10596  SYN_5296 -206826.314286 -381888.542857         55.4\n",
      "11267  SYN_9037 -229686.857143 -119831.371429         54.9\n",
      "10230  SYN_3442 -168908.285714 -186469.485714         51.4\n",
      "9864   SYN_1527 -186951.657143 -241112.200000         49.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n[Churn_Score ìƒì„± ì™„ë£Œ]\")\n",
    "print(f\"ì „ì²´ ê³ ê° ìˆ˜: {len(df_final)}ëª…\")\n",
    "print(f\"ì ìˆ˜ í‰ê· : {df_final['Churn_Score'].mean():.2f}ì \")\n",
    "print(f\"ì ìˆ˜ ìµœëŒ€ê°’: {df_final['Churn_Score'].max()}ì \")\n",
    "\n",
    "print(\"\\n[ìƒìœ„ 5ëª… ë°ì´í„° í™•ì¸]\")\n",
    "cols_view = ['ë°œê¸‰íšŒì›ë²ˆí˜¸', 'Slope_Spend', 'Slope_Balance', 'Churn_Score']\n",
    "print(df_final[cols_view].sort_values('Churn_Score', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d877a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/myenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ë§ ëŒ€ìƒ ë°ì´í„°: 1910ëª…\n",
      "ì‚¬ìš©í•  ë³€ìˆ˜(Feature) ê°œìˆ˜: 862ê°œ\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. ìƒìœ„ 2000ëª…ë§Œ ìƒ˜í”Œë§\n",
    "df_modeling = df_final.head(2000).copy()\n",
    "\n",
    "# 2. í•™ìŠµì— ì‚¬ìš©í•  Featureì™€ Target ì„¤ì •\n",
    "# Target: ìš°ë¦¬ê°€ ë§Œë“  'Churn_Score'\n",
    "# Feature: ìˆ«ìí˜• ë°ì´í„° ì „ë¶€ (ë‹¨, ì‹ë³„ìë‚˜ Target ìì²´ëŠ” ì œì™¸)\n",
    "target_col = 'Churn_Score'\n",
    "ignore_cols = ['ë°œê¸‰íšŒì›ë²ˆí˜¸', 'ê¸°ì¤€ë…„ì›”', target_col]\n",
    "\n",
    "# ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "feature_cols = df_modeling.select_dtypes(include=['number']).columns\n",
    "feature_cols = [c for c in feature_cols if c not in ignore_cols]\n",
    "\n",
    "X = df_modeling[feature_cols]\n",
    "y = df_modeling[target_col]\n",
    "\n",
    "print(f\"ëª¨ë¸ë§ ëŒ€ìƒ ë°ì´í„°: {len(X)}ëª…\")\n",
    "print(f\"ì‚¬ìš©í•  ë³€ìˆ˜(Feature) ê°œìˆ˜: {len(feature_cols)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "954ff1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "# 3. XGBoost íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,     # ë‚˜ë¬´ì˜ ê°œìˆ˜\n",
    "    learning_rate=0.05,   # í•™ìŠµë¥ \n",
    "    max_depth=6,          # ê¹Šì´\n",
    "    random_state=42,\n",
    "    n_jobs=-1             # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©\n",
    ")\n",
    "\n",
    "model.fit(X, y)\n",
    "print(\"XGBoost ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ğŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. SHAP ê°’ ê³„ì‚°\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# 5. ì¤‘ìš”ë„ ìˆœìœ„(Rank) ë§Œë“¤ê¸°\n",
    "# ê° ë³€ìˆ˜ì˜ SHAP ì ˆëŒ€ê°’ í‰ê· ì„ êµ¬í•´ì„œ ì˜í–¥ë ¥ ì¸¡ì •\n",
    "shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'SHAP_Importance': shap_sum\n",
    "})\n",
    "\n",
    "# ì¤‘ìš”ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "importance_df = importance_df.sort_values(by='SHAP_Importance', ascending=False)\n",
    "\n",
    "print(\"\\nğŸ“Š [SHAP Feature Importance Top 10] ğŸ“Š\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# 6. ì‹œê°í™” (ìš”ì•½ ì°¨íŠ¸)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\", show=False)\n",
    "plt.title(\"Top Feature Importance by SHAP Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cc8d56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¹ˆ ê°’ ì²˜ë¦¬ ì™„ë£Œ! (ë°ì´í„° ê°œìˆ˜: 1910ê°œ)\n",
      "ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ğŸš€\n",
      "\n",
      "ğŸ“Š [SHAP Feature Importance Top 10] ğŸ“Š\n",
      "            Feature  SHAP_Importance  Rank\n",
      "0       Slope_Spend         4.299572     1\n",
      "1     Slope_Balance         2.770525     2\n",
      "2       Slope_Count         2.444372     3\n",
      "3        ì”ì•¡_ì¼ì‹œë¶ˆ_B0M         0.007571     4\n",
      "4   ì¦ê°ìœ¨_ì´ìš©ê±´ìˆ˜_ì¼ì‹œë¶ˆ_ë¶„ê¸°         0.007512     5\n",
      "5         ë‚©ë¶€_ê¸°íƒ€ì´ìš©ê¸ˆì•¡         0.006238     6\n",
      "6           ì´ìš©ê¸ˆì•¡_êµí†µ         0.005632     7\n",
      "7        ì •ìƒì…ê¸ˆì›ê¸ˆ_B0M         0.005334     8\n",
      "8        ìŠ¹ì¸ê±°ì ˆê±´ìˆ˜_B0M         0.005210     9\n",
      "9  í• ë¶€ê¸ˆì•¡_ìœ ì´ì_3M_R12M         0.005027    10\n",
      "\n",
      "'VIP_SHAP_Importance.csv' íŒŒì¼ë¡œ ì €ì¥í–ˆì–´! í™•ì¸í•´ë´! ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# 1. ë°ì´í„° ì¤€ë¹„ (ì´ë¯¸ X, yê°€ ë¡œë“œë˜ì–´ ìˆë‹¤ê³  ê°€ì •í• ê²Œ!)\n",
    "# ì˜¤ë¥˜ê°€ ë‚¬ë˜ ë¶€ë¶„ í•´ê²°: ë¹ˆ ê°’(NaN)ì„ 0ìœ¼ë¡œ ì±„ì›Œì£¼ê¸°\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"ë¹ˆ ê°’ ì²˜ë¦¬ ì™„ë£Œ! (ë°ì´í„° ê°œìˆ˜: {len(X)}ê°œ)\")\n",
    "\n",
    "# 2. ëª¨ë¸ í•™ìŠµ\n",
    "# (XGBoost ëŒ€ì‹  sklearnì˜ GradientBoosting ì‚¬ìš© ì¤‘)\n",
    "model = GradientBoostingRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=4, \n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X, y)\n",
    "print(\"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ! ğŸš€\")\n",
    "\n",
    "# 3. SHAP ì§€ìˆ˜ ë½‘ê¸° (ì „ì²´ ë°ì´í„° ëŒ€ìƒ)\n",
    "# TreeExplainerë¥¼ ì‚¬ìš©í•˜ì—¬ SHAP ê°’ ê³„ì‚°\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# 4. ìˆœìœ„ ë° SHAP ì¤‘ìš”ë„ ì •ë¦¬\n",
    "# ê° ë³€ìˆ˜ì˜ SHAP ì ˆëŒ€ê°’ í‰ê· ìœ¼ë¡œ ì¤‘ìš”ë„ ì‚°ì¶œ\n",
    "shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'SHAP_Importance': shap_sum\n",
    "})\n",
    "\n",
    "# ì¤‘ìš”ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ ë° ìˆœìœ„ ë§¤ê¸°ê¸°\n",
    "importance_df = importance_df.sort_values(by='SHAP_Importance', ascending=False).reset_index(drop=True)\n",
    "importance_df['Rank'] = importance_df.index + 1\n",
    "\n",
    "# 5. CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "importance_df.to_csv('VIP_SHAP_Importance.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\nğŸ“Š [SHAP Feature Importance Top 10] ğŸ“Š\")\n",
    "print(importance_df.head(10))\n",
    "print(\"\\n'VIP_SHAP_Importance.csv' íŒŒì¼ë¡œ ì €ì¥í–ˆì–´! í™•ì¸í•´ë´! ğŸ˜Š\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56b2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
